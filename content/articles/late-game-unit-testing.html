---
kind: article
title: Late-game unit testing
tags: [ 'testing', 'test-driven development', 'unit tests', 'old code' ]
created_at: Time.parse( "April 7, 2011" )
author_name: Ryan
excerpt: Sometimes I need to add features to old projects for which I didn't write unit tests. Sometimes I neglect to write unit tests during development in new projects. Oops. In situations like these, what are some approaches to then going back and adding "necessary" test coverage? 
---

I've struggled with this issue recently as I go back and add tests to "sake":http://www.github.com/rgantt/sake, my pet MVC framework. The framework's deployed on enough sites--albeit smallish ones--to confirm that the core functionality is working as intended. When I encounter a framework defect during deployment of a sake site, I revisit the repository and update where necessary. I do "thorough" functional tests to ensure that I haven't introduced any obvious regressions and then continue to hack happily away on the site. 

Of course, there are several problems with this sort of approach. First, even though functional tests don't expose any regression errors, this is not really sufficient evidence to conclude that there _actually_ weren't any regressions. I really don't know whether my routing and request-reading code response the same way to all meaningful input combinations that they did before. I only know that they _probably_ respond identically to _those input combinations I was already using._ In short, one person running functional tests just doesn't stress enough of the system to grab anything but the lowest-hanging fruit.

The second problem is that such _ad hoc_ changes can have an unforeseen impact on The Framework (tm). Modifying framework code in response to bugs exposed on an implementation of that framework is emphatically the wrong way to do it. Since I am developing the sites and the framework simultaneously, I may--even with the best of intentions--apply fixes in a narrow-minded fashion, losing track of the overall design goals of the framework. My incentives are misaligned because of the duality of my role at the time. A healthy suite of unit tests would, in this situation, act as a set of guardrails against such behavior. They would keep me honest. Making naive fixes to get my sites to run properly would trigger unit test failures, forcing me to do things the right way.

Ultimately, this is not intended to be an article about the merits of unit testing. The question I'm wrestling with is what unit tests to _start_ with when you're dealing with an already-completed piece of software that you want to harden against regression. This is inherently a short run concept; in the long run, you'll either have full test coverage for your current functionality, or you'll be left with a dead piece of software and, maybe, revisiting this article! Then again: in the long run, we're all dead. Some of us might prefer that to writing exhaustive tests.

In general, I favor any solution to a problem where the progression tends from highest to lowest value. In unit testing parlance, this suggests that I write tests which cover the simplest, most basic functionality first. Why not? Approaching test-writing this way will tend to harden those parts of your software that are the most basic and thus the most widely-used. Sake requires a lot of URL-generation, so it made sense to start with the @url_rewriter@ for test coverage. I covered the most basic use-cases: making sure that protocols, subdomains, ports, and query strings were properly handled within the application (regardless of the presence of mod_rewrite). Great!

"What next?" This is the question you should be asking every time you create a passing unit test. Does my next-highest value test fit within the context of this test case, or should I start a new one to make sure that my controllers are being loaded properly? There is more marginal value in the short term for adding a test against a widely-used feature than making sure a 4-d array in the query string will map properly into your framework. If the controllers aren't being loaded, your query string isn't going to mean much, anyway! Your first test of controllers means more than your 20th test of URL rewriting in the short term.

What about a situation where a unit test you've written for functionality that you expect to work doesn't pass? At this point, we need to consider whether we're in a position to change framework code. What hat are we wearing? If we're wearing our unit testing hat and we're working against an existing codebase, we might favor altering the tests to get them to pass, rather than altering the framework to implement appropriate functionality. For now. Switching hats too frequently can quickly put you in a situation where your unit test "debt" is accruing faster than you're paying it off. Making changes in one area of the framework quickly leads changes and broken tests in other parts, and we're right back to where we were. Even though it will make the developer within you cringe, set aside the test for now--or simplify it to make it pass--and simultaneously write up the bug. You are using an issue-tracking system, aren't you?!

What now? You've got your basic, highest-value functionality covered well. Your software works as it did before you started writing tests--you haven't changed anything, except for the tests themselves, so there should be no possible way to introduce a regression. You're pretty confident that new features and improvements won't break the framework in unintended ways. You need to take the next step, but you're just not sure what it is.

Now it's time to start revisiting all of those QA defects you wrote up in the previous section and making the quality changes to your framework. You need to be confident enough in your test coverage to know that you aren't adding blatant regressions to your code. You probably don't need 100% coverage (which is a myth, anyway), but you also don't want to rely on your build to tell you whether everything's ok. This is less of "test-driven development" and more of "test-driven test coverage improvements." We're going back through our code and adding coverage that really ought to have been there in the first place. In order to add this coverage, we might need to step in and clean up a few messes.
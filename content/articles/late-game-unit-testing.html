---
kind: article
title: Late-game unit testing
tags: [ 'testing', 'units' ]
created_at: Time.parse( "April 7, 2011" )
author_name: Ryan
excerpt: Sometimes we revisit old projects that don't include unit tests. Sometimes we neglect to write unit tests during development. In situations like these, what is the best approach to then going back and adding the "necessary" test coverage? 
---

I've struggled with this issue recently as I go back and add tests to "sake":http://www.github.com/rgantt/sake, my php mvc framework. I've been using the software on enough sites--albeit smallish ones--to know that the core functionality is working as intended. When I encounter an error during development of a site, I revisit the sake repository and update where necessary. I do functional tests to ensure that I haven't introduced any obvious regressions, and then I continue to hack happily on the original site. There are several problems with this sort of approach, however. First, even though functional tests don't expose any obvious regressions, this is certainly not sufficient evidence to conclude with any confidence that there _actually_ weren't any regressions. I really don't know whether my routing and request-reading code response the same way to all meaningful input combinations that they did before. I only know that they _probably_ respond identically to _those input combinations I was already using._

The second problem is that such _ad hoc_ changes--even though they aren't, strictly speaking, feature additions--can have an impact on the framework in general. Modifying framework code in response to bugs exposed on an implementation of that framework can be subversive. Since I am developing both the site and the framework, I may, even with the best of intentions, apply changes that fix the solution for my current purposes, but leave it exposed in general. My incentives are misaligned because of the duality of my role at the time. A healthy suite of unit tests would, in this situation, act as a set of guardrails against such behavior. They would keep me honest.

Ultimately, this is not intended to be an article about the merits of unit testing. We've all been well-indoctrinated in matters of TDD; the question I'm wrestling with is what unit tests to _start_ with when you're dealing with an already-completed piece of software that you want to harden against regression. This is inherently a short run concept; in the long run, you'll either have full test coverage for your current functionality, or you'll be left with a dead piece of software and, maybe, revisiting this article!

In general, I favor any solution to a problem where the progression tends from simplest to most complicated. In the context of unit testing, this implies that I prefer to write tests that cover the simplest, most basic functionality first. Why not? Approaching test-writing this way will tend to harden those parts of your software that are the most basic (and thus the most widely-used). Sake requires a lot of URL-generation, so it made sense to start with the @url_rewriter@ for test coverage. I covered the most basic use-cases: making sure that protocols, subdomains, ports, and query strings were properly handled within the application (regardless of the presence of mod_rewrite). Great!

"Now what?" This is the question you should be asking every time you created a passing unit test. Is my next-simplest test in this case test case, or should I start a new test case to make sure that my controllers are being loaded properly? There is more marginal value in the short term for adding a test against a widely-used feature than making sure a 4-d array in the query string will map properly into your framework. If the controllers aren't being loaded, your query string isn't going to mean much, anyway! Your first test of controllers means more than your 20th test of URL rewriting in the short term. Of course, none of this is particularly contentious.

What about a situation where a unit test you've written doesn't pass? At this point, we need to consider whether we're in a position to change framework code. What hat are we wearing? If we're wearing our unit testing hat and we're working against an existing codebase, we might favor altering the tests to get them to pass, rather than altering the framework to implement appropriate functionality. Cross this line too many times and you may end up in a situation where you're crossing into untested territory. Fixing concepts in one area of the framework quickly leads to other parts, and we're right back to where we were. Even though it will make the developer within you cringe, set aside the test for now--or simplify it to make it pass--and simultaneously write up the bug. You are using issue-tracking, aren't you?!

What now? Say you've got your basic functionality covered well. Your software works as it did--you haven't changed anything, except for the tests themselves, so there should be no possible way to introduce a regression. When is it time to make the transition from post-hoc test coverage to test-driven development? This is a slightly less well-defined subject area. Sometimes writing tests for existing functionality will lead to ideas for new functionality within existing units. To be sufficiently vague, it's time to start revisiting all of those QA defects you wrote up in the previous section and making the quality changes to your framework. You need to be confident enough in your test coverage to know that you aren't adding blatant regressions to your code. You probably don't need 100% coverage (which is a myth, anyway), but you also don't want to rely on your build to tell you whether everything's ok. This is less of "test-driven development" and more of "test-driven test coverage improvements." We're going back through our code and adding coverage that really ought to have been there in the first place. In order to add this coverage, we might need to step in and clean up a few messes.